---
layout:     post
title:      "Articles of Pooling"
subtitle:   "Papers from AAAI, IJCAI, ACL and EMNLP"
date:       2018-1-6 21:00:00
author:     "Procjx"
header-img: "img/blog/machine-learning.jpg"
catalog:    true
tags:
    - Pooling 
    
---

#### 《Weakly Supervised Semantic Segmentation》
###### Suha Kwak et al., AAAI, 2017

- <b>摘要</b><br/>
&nbsp; We propose a weakly supervised semantic segmentation algorithm based on deep neural networks, which relies on imagelevel
class labels only. The proposed algorithm alternates between generating segmentation annotations and learning a semantic segmentation network using the generated annotations. A key determinant of success in this framework is the capability to construct reliable initial annotations given image-level labels only. To this end, we propose Superpixel Pooling Network (SPN), which utilizes superpixel segmentation of input image as a pooling layout to reflect low-level image structure for learning and inferring semantic segmentation. The initial annotations generated by SPN are then used to learn another neural network that estimates pixelwise semantic labels. The architecture of the segmentation network decouples semantic segmentation task into classification and segmentation so that the network learns classagnostic shape prior from the noisy annotations. It turns out that both networks are critical to improve semantic segmentation accuracy. The proposed algorithm achieves outstanding performance in weakly supervised semantic segmentation task compared to existing techniques on the challenging PASCAL VOC 2012 segmentation benchmark.
 <br/>
 
- [PDF](https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/viewFile/14445/14288)

---

#### 《Earth Mover's Distance Pooling over Siamese LSTMs for Automatic Short Answer Grading》
###### Sachin Kumar et al., IJCAI, 2017
- <b>摘要</b><br/>
&nbsp; Automatic short answer grading (ASAG) can reduce tedium for instructors, but is complicated by free-form student inputs. An important ASAG task is to assign ordinal scores to student answers, given some “model” or ideal answers. Here we introduce a novel framework for ASAG by cascading three neural building blocks: Siamese bidirectional LSTMs applied to a model and a student answer, a novel pooling layer based on earth-mover distance (EMD) across all hidden states from both LSTMs, and a flexible final regression layer to output scores. On standard ASAG data sets, our system shows substantial reduction in grade estimation error compared to competitive baselines. We demonstrate that EMD pooling results in substantial accuracy gains, and that a support vector ordinal regression (SVOR) output layer helps outperform softmax. Our system also outperforms recent attention mechanisms on LSTM states. 
 <br/>
 
- [PDF](https://www.ijcai.org/proceedings/2017/0284.pdf)

---

#### 《Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding》
###### Akira Fukui et al., EMNLP, 2016
- <b>摘要</b><br/>
&nbsp; Modeling textual or visual information with vector representations trained from large language or visual datasets has been successfully explored in recent years. However, tasks such as visual question answering require combining these vector representations with each other. Approaches to multimodal pooling include element-wise product or sum, as well as concatenation of the visual and textual representations. We hypothesize that these methods are not as expressive as an outer product of the visual and textual vectors. As the outer product is typically infeasible due to its high dimensionality, we instead propose utilizing Multimodal Compact Bilinear pooling (MCB) to efficiently and expressively combine multimodal features. We extensively evaluate MCB on the visual question answering and grounding tasks. We consistently show the benefit of MCB over ablations without MCB. For visual question answering, we present an architecture which uses MCB twice, once for predicting attention over spatial features and again to combine the attended representation with the question representation. This model outperforms the state-of-the-art on the Visual7W dataset and the VQA challenge.
 <br/>
 
- [PDF](http://www.aclweb.org/anthology/D16-1044)

---

#### 《Text Classification Improved by Integrating Bidirectional LSTM》
###### Bo Xu Yin et al., COLING, 2016
- <b>摘要</b><br/>
&nbsp; Recurrent Neural Network (RNN) is one of the most popular architectures used in Natural Language Processsing (NLP) tasks because its recurrent structure is very suitable to process variablelength text. RNN can utilize distributed representations of words by first converting the tokens comprising each text into vectors, which form a matrix. And this matrix includes two dimensions: the time-step dimension and the feature vector dimension. Then most existing models usually utilize one-dimensional (1D) max pooling operation or attention-based operation only on the time-step dimension to obtain a fixed-length vector. However, the features on the feature vector dimension are not mutually independent, and simply applying 1D pooling operation over the time-step dimension independently may destroy the structure of the feature representation. On the other hand, applying two-dimensional (2D) pooling operation over the two dimensions may sample more meaningful features for sequence modeling tasks. To integrate the features on both dimensions of the matrix, this paper explores applying 2D max pooling operation to obtain a fixed-length representation of the text. This paper also utilizes 2D convolution to sample more meaningful information of the matrix. Experiments are conducted on six text classification tasks, including sentiment analysis, question classification, subjectivity classification and newsgroup classification. Compared with the state-of-the-art models, the proposed models achieve excellent performance on 4 out of 6 tasks. Specifically, one of the proposed models achieves highest accuracy on Stanford Sentiment Treebank binary classification and fine-grained classification tasks.
 <br/>
 
- [PDF](http://www.aclweb.org/anthology/C16-1329)

---

#### 《Event Extraction via Dynamic Multi-Pooling Convolutional Neural》
###### Kang Liu et al., ACL, 2015
- <b>摘要</b><br/>
&nbsp; Traditional approaches to the task of ACE event extraction primarily rely on elaborately designed features and complicated natural language processing (NLP) tools. These traditional approaches lack generalization, take a large amount of human effort and are prone to error propagation and data sparsity problems. This paper proposes a novel event-extraction method, which aims to automatically extract lexical-level and sentence-level features without using complicated NLP tools. We introduce a word-representation model to capture meaningful semantic regularities for words and adopt a framework based on a convolutional neural network (CNN) to capture sentence-level clues. However, CNN can only capture the most important information in a sentence and may miss valuable facts when considering multiple-event sentences. We propose a dynamic multi-pooling convolutional neural network (DMCNN), which uses a dynamic multi-pooling layer according to event triggers and arguments, to reserve more crucial information. The experimental results show that our approach significantly outperforms other state-of-the-art methods. 
 <br/>
 
- [PDF](http://www.aclweb.org/anthology/P15-1017)

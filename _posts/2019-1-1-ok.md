---
layout:     post
title:      "【论文笔记】Unsupervised Neural Single-Document Summarization of Reviews via Learning Latent Discourse Structure and its Ranking"
subtitle:   "Papers from AAAI, IJCAI, ACL and EMNLP 2017"
date:       2017-11-27 10:00:00
author:     "Procjx"
header-img: "img/blog/machine-learning.jpg"
catalog:    true
tags:
    - Sentence Classification
    - Document Classification
    - Articles
    
---
**Unsupervised Neural Single-Document Summarization of Reviews via Learning Latent Discourse Structure and its Ranking**. Masaru Isonuma, Junichiro Mori, Ichiro Sakata. ACL 2019. [\[PDF\]](https://arxiv.org/pdf/1906.05691.pdf)

# 动机

本文认为，评论（review）可以当作一个棵篇章树，树的根节点是其摘要，表示该评论的整体意思; 树的其他节点是对其父节点的细化。 也就是说这棵篇章树由摘要（根节点）与评论中所有句子（非根节点，每个非根节点代表一个句子）组成。于是本文通过学习构造这个隐式篇章树来建模得到评论摘要，并提出一种排序（rank）算法选择对生成摘要更加重要的句子。

![](https://img-blog.csdnimg.cn/20191030175132848.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2hhaXRhb2xhbmc=,size_16,color_FFFFFF,t_70)

# 方法

## 模型整体方法

（1）双向GRU+maxpooling 建模得到每个句子表示

（2）建模 父节点-子节点 对应关系权重（权重代表了树的关系）

（3）加权求和所有子节点表示，生成父节点（本文假设：子节点能够还原父节点，因为子节点包含了比父节点更多的信息。）

目标函数就是所有父节点生成概率最大。

![](https://img-blog.csdnimg.cn/20191030180138199.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2hhaXRhb2xhbmc=,size_16,color_FFFFFF,t_70)


 

## 父节点-子节点 对应关系权重建模方法

初始建模：边界概率（Marginal Probability of Dependency）

![图片](https://img-blog.csdnimg.cn/20191030180811147.png)
![](https://img-blog.csdnimg.cn/20191030180811147.png)

归一化（公式推导不是很懂）

![](https://img-blog.csdnimg.cn/20191030180849529.png)


 

调整：篇章排序（DiscourseRank）

受PageRank算法启发，更重要的句子有更多后代，迭代更新权重矩阵。
![](https://img-blog.csdnimg.cn/20191030180931300.png)
![](https://img-blog.csdnimg.cn/20191030181000690.png)
